<div id="textbox">
	<h2> Projects </h2>
	<div id="subtextbox">
	<!-- Add Information Here-->
	<p>Our mission is to study, design and develop innovative systems and technologies that work at the leading edge of human and computer interaction. Our goal is to create systems that work beyond the lab to enable rich interactive learning in a variety of applications for a broad and diverse range of users in everyday, real world settings.  This involves empirical exploratory/developmental research that leverages and integrates advances across Engineering, Computer Science, Psychology, Linguistics, Education, Speech Language and Hearing, and Health Sciences. 
	</p>
	<h3>Multimedia multimodal human-computer interaction</h3>
	<ul>
		<li>Development and evaluation of intelligent, assistive, and informational interfaces</li>
		<li>Study and implementation of conversational animated agents, study of affect</li>
		<li>Technologies and approaches for personalized learning, collaboration, and communication</li>
		<li>Development and implementation of technologies for interactive web and mobile applications</li>
	</ul>
	<h3>Speech and language engineering</h3>
	<ul>
		<li>Application of speech and speaker verification, speech synthesis, and dialog interaction Computational, statistical, and empirical methods in speech, phonetics, and linguistics</li>
		<li>Auditory signal processing</li>
	</ul>
	<h3>Applications for interactive learning</h3>
	<ul>
		<li>Computer instruction and assessment for reading, literacy and science</li>
		<li>Computer assistance for workforce training</li>
		<li>Computer remediation of speech, language, and cognitive disabilities</li>
	</ul>

	<hr size="1">
	<a name="animated_agent">
	<img src="images/agent.png" align="right"/>
	<h2> Animated Agent with Accurate Visible Speech for Learning on the Web </h2>
	Funded by  the University of Colorado Graduate School (Innovative Grant Program), 2008-2009
	<h4>Sarel van Vuuren (PI), Barbara Wise (Co-PI)</h4>
	<p>This project develops an animated agent with accurate visual speech that can be used to guide and assist learning interactively on the Web (Van Vuuren et al., 2008). As much as a third of the U.S. population across the lifespan read at or below the basic level. Millions have cognitive disabilities that make it difficult for them to access more than simple text in simple contexts (NAAL, 2003; NAEP,2007;  AOA, 2007,Braddock, 2007). Accurate intelligent agents promise broad benefits: they can enable a wide range of interactive applications in education, healthcare, workforce training, information assistance and communications, and basic research in these areas in cognition, perception, and communication.
	</p>
	<p>To develop the agent, we take a multidisciplinary approach that leverages expertise and innovations in articulatory phonetics, computer graphics, signal processing, and software engineering. The result is a high-fidelity animated agent with accurate visual speech and a range of facial expressions and head movements that can run as a Web application on virtually any computer or mobile device. Implementations can run in stand-alone, online and mobile configurations (currently Google Android) on Flash, Java or HTML 5 compatible web browsers - ideal for Web 2.0 and Cloud applications.  Implementations on other mobile devices, including the Apple iPhone, iPod touch, Nintendo DSi and e-readers, are also feasible.
	</p>
	<p><b>Major scientific and technical outcomes of the project are a novel variable rate composite articulatory model of visual speech; a highly compact and portable implementation; and new multi-disciplinary research collaborations (Van Vuuren, et al, 2009).</b>
	</p>
	<p><b>Personnel:</b> Nattawut Ngampatipatpong, Robert Bowen, Taylor Struemph
	</p>

	<hr size="1">
	<a name="carti">
	<img src="images/carti.png" align="right"/>
	<h2> Computer-Assisted Response to Instruction for Reading Difficulties (CARTI) </h2>
	Funded by the National Institutes of Health, 2005-2010
	<h4>Barbara Wise (PI), Sarel van Vuuren, Brian Byrne (Co-PIs)</h4>
	<p>This project uses classroom-based "Tier 2" computer instruction to study questions about response to intervention (RTI) with children who have been assessed in the prior year to have reading difficulties. CARTI is Project V of the Colorado Learning Disabilities Research Center in the Institute for Behavioral Genetics (Richard Olson, PI), which studies variations in RTI and whether initial assessments or rate of response better predict outcomes. Children who progress slowly with the intervention are given extra intensive "Tier 3" instruction in a second year, where they help with participatory design to make more intensive activities to improve early learning of letters and sound, or later problems with fluency, vocabulary, and comprehension.
	</p>
	<p>The project combines 1) effective practices in integrated and supportive reading instruction (Wise, Ring & Olson, 2000), 2) scientific reading research (McCardle & Chhabras, 2004; Rayner et al, 2001) 3) an extensive amount of highly sequenced and scaffolded reading material, 4) numerous activities and ways to practice phonological awareness, word reading, spelling, vocabulary, fluent reading and comprehension (NRP, 2000), and 5) an animated learning assistant with accurate visible speech, that 6) provides focused hints and encourage learning in a self-paced, performance-oriented environment (Wise, Van Vuuren, Ngampatipatpong, 2008).
	</p>
	<p>
	Major scientific outcomes of the project include a computer-based reading intervention for KG-5th graders called CARLA that can augment regular instruction with repeated assessment, individualization, and great fidelity of treatment -making it particularly well-suited for studying questions about RTI. CARLA has a comprehensive learning environment featuring more than 30 different activities across 110 levels (Wise, Van Vuuren, Ngampatipatpong, 2008). In the RTI study, Cohort 1 lost 1 school but shows good results on high stakes tests along with praise from teachers in these and preliminary continuing schools. Post-testing is just completed, to be analyzed this summer. When Cohort 2 runs next year we will have more power for growth analyses and comparisons of slow and faster learner's initial cognitive and genetic profiles. 
	</p>
	<p><b>ISL's technical and computational contributions to the project include methods for: deriving a highly structured study plan with more than 6,000 rhyming sequences, 40,000 items and 300,000 phonologically and orthographically close distracters (Van Vuuren et al, 2008); repeated review and dynamic assessment of word reading  (Van Vuuren et al, 2009); performance monitoring and reporting (Van Vuuren, 2007); animated agent; speech prompting and alignment; user interfaces; and scalable robust software implementation.</b>
	</p>
	<p><b>Personnel:</b> Nattawut Ngampatipatpong, Jariya Tuantranont, Taylor Struemph, Micaela Christopher, Robert Bowen, Tammy Tomczyk, Luann Sessions, Tim Weston, Gordon Golding, Laurell Richey, Joel Terrel, Jackie Montoya
	</p>

	<hr size="1">
	<a name="icare">
	<img src="images/icare.png" align="right"/>
	<h2> ICARE: Independent Comprehensive Adaptive Reading Evaluation </h2>
	Funded by the Institute of Education Sciences, 2004-2009
	<h4>Barbara Wise (PI), Sarel van Vuuren (Co-PI)</h4>
	Lynn Snyder, Tim Weston, Richard Olson (Key personnel)
	<p>This project develops a computer-based system for independent comprehensive adaptive reading evaluation (ICARE) of children in 2nd  to 5th grade. Large numbers of children in the US struggle to read (33% of 4th graders in 2007) and being able to quickly screen and provide them with an initial instructional profile can save teachers tremendous time in helping them get the help they need. ICARE is theoretically grounded in the simple view of reading (Hoover & Gough, 1990) The simple view describes reading comprehension as a product of efficient word reading and listening comprehension. For its major assessments of word reading, ICARE uses item response theory (IRT) to select and order items. ICARE takes a novel approach to measuring reading comprehension. Most comprehension measures have one of two problems. Some are highly reliable by virtue of many short passages, but these turn out to be invalid and measure only word recognition (Keenan & Olson, 2006). Others with longer passages suffer from confounds with background knowledge; children may be able to answer most of the questions without needing to read the passages at all (Keenan & Betjemann, 2006), ICARE is circumventing this problem with "informational passages" about novel universes, long enough that a child needs to build up a situation model or gist for deep comprehension, but short enough for efficiency. All carefully designed gist-based questions and distracters are based on novel content (Wise, Snyder, Weston, Sessions, & Sager, in press). These passages are used both for reading and listening comprehension.
	</p>
	<p>The project makes several major scientific contributions to advance computerized assessment of children's reading. It provides independence by delivering assessments and items with high-fidelity using a conversational animated assistant. It adaptively and comprehensively builds an instructional profile using a variety of untimed and time-limited word level assessments some with speech and choice options, and language comprehension and  vocabulary assessments . It shows strong concurrent validity compared to traditional paper-based assessments.
	</p>
	<p><b>ISL's technical and computational contributions to the project include speech technologies, probabilistic models for exiting, animated agent, and computer software implementation.</b>
	</p>
	<p><b>Personnel:</b> Luann Sessions, Tammy Tomczyk, Nattawut Ngampatipatpong, Jariya Tuantranont, Taylor Struemph, Irene Faivre, Nicole Sager, Robert Bowen, Gordon Golding
	</p>

	<hr size="1">
	<a name="early_icare">
	<img src="images/earlyicare.png" align="right"/>
	<h2> EARLY ICARE: Early Independent Comprehensive Adaptive Reading Evaluation </h2>
	Funded by the Institute of Education Sciences, 2007-2011
	<h4>Barbara Wise (PI), Sarel van Vuuren, Lynn Snyder, Ed Wiley (Co-PIs)</h4>
	<p>This project extends ICARE to younger readers by developing a computer-based system for independent comprehensive adaptive reading evaluation (Early-ICARE) for children in KG to 3rd grade.  Large numbers of children in the US emerge as struggling readers in 4th grade (33% of 4th graders NAEP 2007). Providing efficient screening with an outcome of an initial instructional profile at an young age could really help teachers provide these with the instruction they need at an early age. Early-ICARE is grounded in the simple view of reading (Hoover & Gough, 1990). It includes independent dynamic measures to be used in response to intervention (RTI) models of delivery (Vaughn & Fuchs, 2003). For major assessments of word reading, items in Early-ICARE are selected and ordered using item response theory (IRT) and delivered using Bayesian adaptive techniques, making for efficient and cost-effective assessment. As with ICARE, Early-ICARE is independent because assessments are delivered independently by computer using a conversational animated assistant.
	</p>
	<p>The project makes several contributions to advance computerized assessment of early reading. It includes new automatic measures of letter and letter-sound knowledge, rapid object and color naming,  for young readers, and measures of motivation. Spelling measures and measures of phonological memory are in development. 
	</p>
	<p><b>ISL's computational contributions add: automatic measures of speech rate and pausing during fluent reading, and repeated independent dynamic measures of reading that can be embedded conveniently within an intervention to provide an RTI profile and or adapt instruction to children's individual needs. The dynamic measures include 50 forms and 1500 items computed and sequenced automatically with preliminary data indicating reliabilities of 0.8-0.9 and concurrent validity compared to standardized paper-based assessments.</b>
	</p>
	<p><b>Personnel:</b> Nattawut Ngampatipatpong, Jariya Tuantranont, Taylor Struemph, Robert Bowen, Tammy Tomczyk, Luann Sessions, Gordon Golding, Tim Weston
	</p>

	<hr size="1">
	<a name="workforce">
	<img src="images/workforce.png" align="right"/>
	<h2> Perceptive Animated Interfaces for Workforce Training </h2>
	Funded by the National Institute on Disability and Rehabilitation Research, 2004-2009
	<div id="subtextbox">
	<!-- Add Information Here-->
	<h4>Sarel van Vuuren (Co-PI, UCB), Judy Emery, Jim Sandstrum (Co-PIs, UCD)</h4>
	<p>This project develops a job previewing system to support people with cognitive disabilities as they enter the workforce. This is a collaborative project (D5) within the larger RERC on Advancement of Cognitive Technologies, Cathy Bodine, overall PI and Director, ATP, CU Denver School of Medicine, and Michael Lightner, Co-PI and Chair, Department ECEE, CU Boulder. The RERC-ACT strives to improve the quality of life for individuals with cognitive disabilities through research and development of new cognitive technologies (Bodine & Lightner, 2008).
	</p>
	<p>
	For job seekers with cognitive disabilities, it can be a challenge to learn about opportunities and accommodations in the workplace. The project aims to study objective and subjective factors that can assist them. Of interest are measures of task completion, verbal and gestural interaction, frequency, manner and ease by which application and informational elements are accessed, as well as motivational, emotional, goal and cognitive oriented performance measures. 
	</p>
	<p>We have developed a prototype multimedia learning system which focuses on mail room and file clerk jobs commonly found in a professional office setting. The prototype uses an animated learning assistant to help users examine and identify jobs that are both interesting and suited to their abilities. It offers rich previews of occupations with video, images, speech, and Flash animations (Emery & Sandstrum, 2008). The prototype was recently tested at local support centers for the cognitively disabled with data analyses ongoing.
	</p>
	<p><b>Personnel:</b> Taylor Struemph, Nattawut Ngampatipatpong, Jariya Tuantranont
	</p>
	<br><br>

	<hr size="1">
	<a name="science_learning">
	<h2> Improving Science Learning in Inquiry-based Programs </h2>
	Funded by the National Science Foundation, 2007-2012
	<h4>Sarel van Vuuren (PI)</h4>
	<p>The project is one of two separate collaborative NSF grants (Sarel van Vuuren, PI; and Wayne Ward, PI). Our project (Van Vuuren, PI) designs, develops, and optimizes dialog interaction and speech recognition technologies for a computer-based science tutor.  The goal is to move beyond simple form filling, multiple choice and list negotiation to spoken dialog interaction (Van Vuuren et al., 2008; Van Vuuren & Ward, 2008). The context for the project is 4th and 5th grade struggling science learners who, following classroom instruction, are tested for conceptual knowledge using multiple choice questions and then asked to reason about and reinforce their procedural knowledge of science concepts in one-on-one tutoring sessions. Project materials are built around the high-quality inquiry-based FOSS science program (www.fossweb.com). The theoretical motivation for the project is to emulate an open-ended tutorial dialog strategy similar to the 'Questioning the Author' technique in which students are asked to think about and integrate recently learned science concepts to develop rich mental models of the science material (Beck et al, 1996; McKeown et al, 1999).
	</p>
	<p>
	The collaborative project (Ward, PI) is advising speech and dialog integration, developing curriculum, multimedia materials, collecting data, and testing the system in Boulder Valley schools. The system builds on an interactive learning application environment developed by Ngampatipatpong and Van Vuuren (Van Vuuren, 2007). The science tutor will be evaluated by comparing learning gains on standardized assessments with students randomly assigned to computer treatment, human tutoring and continued classroom instruction groups. Successful outcomes of the project will further a technological framework for learning interventions by elementary school students using computer-based spoken dialog interaction.
	</p>
	<p><b>Project contributions are in three main areas 1) Speech recognition technologies to model the variable and colloquial nature of children's speech and transcribe their spoken responses to open-ended questions, 2) Dialog interaction technologies to map student responses into contextual structures that can be used to track their progress and react accordingly, and 3) Annotation of topic related entities, concepts and misconceptions in transcribed student responses to model student's misconceptions in science, assess their understanding, measure coverage of subject material, and model words and phrases students use when talking about science.</b>
	</p>
	<p><b>Personnel:</b> Tim Weston, Kathy Garvin-Doxas
	<br>Lee Becker, Jing Zheng (Graduate Research Assistants)
	<br>Daniel Bolanos (Post doc)
	</p>
	<hr size="1">
	</div>
</div>			
